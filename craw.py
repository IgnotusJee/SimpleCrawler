#!/usr/bin/env python3
"""
Enhanced Batch Web Scraper with Anti-Anti-Scraping Features
"""
import os
import re
import sys
import aiohttp
import asyncio
import hashlib
import sqlite3
import random
from datetime import datetime, timedelta
from urllib.parse import urlparse
from typing import Iterator, Dict, Any, Optional
from itertools import product
import logging
from logging.handlers import RotatingFileHandler
import argparse
import json
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
from playwright.async_api import async_playwright

DEFAULT_CONFIG = {
    'concurrency': 5,
    'retry_policy': {
        'max_attempts': 3,
        'backoff_base': 2
    },
    'storage': {
        'base_dir': './downloads',
        'db_file': 'scraped_data.db',
        'max_file_size': 1024 * 1024 * 100
    },
    'logging': {
        'log_filename': 'scraper.log',
        'backup_count': 5,
        'max_bytes': 1024 * 1024 * 10
    },
    'file_types': {
        'images': ['jpg', 'jpeg', 'png', 'gif', 'webp'],
        'documents': ['pdf', 'doc', 'docx', 'xls', 'xlsx', 'ppt', 'pptx'],
        'archives': ['zip', 'rar', 'gz'],
        'webviews': ['html', 'php', 'css'],
        'texts': ['json', 'txt'],
    },
    'resource_patterns': [
        r'.*\.(jpg|jpeg|png|gif|webp)(\?.*)?$',
        r'.*\.(pdf|doc|docx|xls|xlsx)(\?.*)?$'
    ],
    'browser': {
        'timeout': 30000,  # 30ç§’è¶…æ—¶
        'max_pages': 3  # æœ€å¤§å¹¶å‘é¡µé¢æ•°
    },
    'request_delay': [0.5, 3.0],  # éšæœºå»¶è¿ŸèŒƒå›´
    'proxies': [],  # ä»£ç†æœåŠ¡å™¨åˆ—è¡¨
    'headers': {  # åŸºç¡€è¯·æ±‚å¤´
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.9',
        'Accept-Encoding': 'gzip, deflate, br',
        'Referer': 'https://www.google.com/',
        'DNT': '1',
        'Connection': 'keep-alive',
    }
}


class URLGenerator:
    """åŠ¨æ€URLç”Ÿæˆå™¨"""

    def __init__(self, config: Dict[str, Any]):
        """config: åŒ…å«æ¨¡æ¿å’Œç”Ÿæˆè§„åˆ™çš„å­—å…¸"""
        self.template = config['url_template']
        self.rules = config['variables']
        self.fixed = False
        self._validate_config()

    def _validate_config(self):
        """éªŒè¯é…ç½®å®Œæ•´æ€§"""
        # æå–æ¨¡æ¿å˜é‡
        variables = set(re.findall(r'\{(.*?)}', self.template))

        if len(variables) == 0:
            self.fixed = True
            return

        # éªŒè¯è§„åˆ™åŒ¹é…
        missing = variables - set(self.rules.keys())
        if missing:
            raise ValueError(f"ç¼ºå°‘å˜é‡è§„åˆ™: {missing}")

        # éªŒè¯è§„åˆ™å‚æ•°
        required_params = {'type', 'start', 'end', 'step'}
        for var, rule in self.rules.items():
            if missing_params := required_params - set(rule.keys()):
                raise ValueError(f"å˜é‡ {var} ç¼ºå°‘å‚æ•°: {missing_params}")

    def _gen_numbers(self, rule: Dict) -> Iterator[int]:
        """ç”Ÿæˆæ•°å€¼åºåˆ—"""
        current = rule['start']
        end = rule['end']
        step = rule['step']

        while current <= end:
            yield current
            current += step

    def _gen_dates(self, rule: Dict) -> Iterator[str]:
        """ç”Ÿæˆæ—¥æœŸåºåˆ—"""
        format = str(rule['format'])
        start = datetime.strptime(str(rule['start']), format)
        end = datetime.strptime(str(rule['end']), format)
        step = timedelta(days=rule['step'])

        current = start
        while current <= end:
            yield current.strftime(format)
            current += step

    def _generate_values(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ‰€æœ‰å˜é‡çš„å€¼åºåˆ—"""
        value_generators = {}

        for var, rule in self.rules.items():
            if rule['type'] == 'number':
                value_generators[var] = self._gen_numbers(rule)
            elif rule['type'] == 'date':
                value_generators[var] = self._gen_dates(rule)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„å˜é‡ç±»å‹: {rule['type']}")

        return value_generators

    def __iter__(self) -> Iterator[str]:
        """ç”Ÿæˆå®Œæ•´çš„URLåºåˆ—"""
        if self.fixed:
            yield self.template
            return

        values = self._generate_values()

        # ç”Ÿæˆç¬›å¡å°”ç§¯
        all_combinations = product(*[
            list(gen) for gen in values.values()
        ])

        for combination in all_combinations:
            params = dict(zip(values.keys(), combination))
            yield self.template.format(**params)


class AsyncScraper:
    """å¼‚æ­¥æŠ“å–å¼•æ“ï¼ˆå¢å¼ºåçˆ¬èƒ½åŠ›ç‰ˆï¼‰"""

    def __init__(self, config: Dict[str, Any], timestamp, force_render):
        self.playwright = None
        self.config = config
        self.timestamp = timestamp
        # æ˜¯å¦å¼ºåˆ¶ç½‘é¡µæ¸²æŸ“
        self.force_render = force_render

        # åˆå§‹åŒ–æµè§ˆå™¨æŒ‡çº¹
        self.ua = UserAgent()
        self.browser = None
        self.browser_sem = None

        # åˆå§‹åŒ–å¼‚æ­¥ç»„ä»¶
        self.session = None
        self.semaphore = asyncio.Semaphore(config['concurrency'])
        self.logger = self.setup_logger()
        self.db_conn = sqlite3.connect(config['storage']['db_file'])
        self.init_db()
        self.queue = asyncio.Queue(maxsize=1000)
        # ç”¨æˆ·ä»£ç†è½®æ¢ç¼“å­˜
        self._current_headers = None

    def init_db(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        with self.db_conn:
            self.db_conn.execute('''
				CREATE TABLE IF NOT EXISTS pages (
					id INTEGER PRIMARY KEY,
					url TEXT,
					content_hash TEXT,
					file_path TEXT,
					timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
					status_code INTEGER,
					attempts INTEGER,
					content_type TEXT
				)
			''')
            self.db_conn.execute('''
				CREATE INDEX IF NOT EXISTS idx_content_type ON pages (content_type)
			''')

    def setup_logger(self):
        """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
        logger = logging.getLogger('BatchScraper')
        logger.setLevel(logging.INFO)

        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )

        file_handler = RotatingFileHandler(
            self.config['logging']['log_filename'],
            maxBytes=self.config['logging']['max_bytes'],
            backupCount=self.config['logging']['backup_count']
        )
        file_handler.setFormatter(formatter)

        logger.addHandler(file_handler)
        return logger

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†"""
        await self.init_browser()
        return self

    async def __aexit__(self, exc_type, exc, tb):
        """æ¸…ç†èµ„æº"""
        await self.close()

    async def init_browser(self):
        """åˆå§‹åŒ–æ— å¤´æµè§ˆå™¨"""
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(
            headless=True,
            proxy={
                'server': self._random_proxy()
            } if self.config['proxies'] else None
        )
        self.browser_sem = asyncio.Semaphore(
            self.config['browser']['max_pages']
        )

    async def close(self):
        """å…³é—­æ‰€æœ‰èµ„æº"""
        if self.session:
            await self.session.close()
        if self.browser:
            await self.browser.close()
        if hasattr(self, 'playwright'):
            await self.playwright.stop()
        self.db_conn.close()

    def _random_proxy(self) -> Optional[str]:
        """éšæœºè·å–ä»£ç†æœåŠ¡å™¨"""
        return random.choice(self.config['proxies']) if self.config['proxies'] else None

    def _gen_headers(self) -> Dict[str, str]:
        """ç”ŸæˆåŠ¨æ€è¯·æ±‚å¤´"""
        base = {
            'User-Agent': self.ua.random,
            **self.config['headers']
        }
        # è‡ªåŠ¨æ·»åŠ Refereré“¾
        if self._current_headers and 'Referer' in self._current_headers:
            base['Referer'] = self._current_headers['Referer']
        return base

    async def fetch(self, url: str, is_resource: bool) -> Dict[str, Any]:
        """æ™ºèƒ½è¯·æ±‚æ–¹æ³•"""
        # éšæœºå»¶è¿Ÿ
        delay = random.uniform(*self.config['request_delay'])
        await asyncio.sleep(delay)

        if not is_resource and self.force_render:
            result = await self._browser_fetch(url)
        else:
            # ä¼˜å…ˆå°è¯•APIè¯·æ±‚
            result = await self._http_fetch(url)
            # éœ€è¦æµè§ˆå™¨æ¸²æŸ“çš„æƒ…å†µ
            if not is_resource and self._needs_browser_rendering(result):
                result = await self._browser_fetch(url)

        return result

    async def _http_fetch(self, url: str) -> Dict[str, Any]:
        """ä½¿ç”¨aiohttpè¿›è¡Œè¯·æ±‚"""
        headers = self._gen_headers()
        self._current_headers = headers  # ä¿å­˜å½“å‰headers
        async with self.semaphore:
            for attempt in range(self.config['retry_policy']['max_attempts']):
                try:
                    async with self.session.get(
                            url,
                            headers=headers,
                            proxy=self._random_proxy(),
                            timeout=aiohttp.ClientTimeout(total=30),
                            ssl=False
                    ) as response:
                        content = await self._process_response(response)
                        return self._success_result(url, content, response)

                except Exception as e:
                    await self._handle_error(url, e, attempt)
            return self._failure_result(url)

    async def _browser_fetch(self, url: str) -> Dict[str, Any]:
        """ä½¿ç”¨æ— å¤´æµè§ˆå™¨è·å–é¡µé¢"""
        async with self.browser_sem:
            page = None
            try:
                context = await self.browser.new_context(
                    user_agent=self.ua.random,
                    locale='en-US',
                    timezone_id='America/New_York',
                )
                page = await context.new_page()

                # è®¾ç½®æµè§ˆå™¨æŒ‡çº¹
                await self._set_browser_fingerprint(page)
                await page.goto(url, timeout=self.config['browser']['timeout'])

                # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
                await page.wait_for_load_state('networkidle')

                # è·å–æœ€ç»ˆå†…å®¹
                content = await page.content()
                return self._success_result(url, content)

            except Exception as e:
                self.logger.error(f"Browser fetch failed: {str(e)}")
                return self._failure_result(url)
            finally:
                if page:
                    await page.close()
                if context:
                    await context.close()

    async def _set_browser_fingerprint(self, page):
        """è®¾ç½®æµè§ˆå™¨æŒ‡çº¹"""
        # è¦†ç›–webdriverå±æ€§
        await page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            })
        """)
        # è®¾ç½®å±å¹•åˆ†è¾¨ç‡
        await page.set_viewport_size({"width": 1920, "height": 1080})

    async def _process_response(self, response) -> Any:
        """å¤„ç†å“åº”å†…å®¹"""
        content_type = response.headers.get('Content-Type', '').split(';')[0]
        if 'text' in content_type:
            return await response.text()
        return await response.read()

    def _needs_browser_rendering(self, result: Dict) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æµè§ˆå™¨æ¸²æŸ“"""
        if result.get('status') != 200 or result.get('content_type', '') != 'text/html':
            return False
        # æ£€æµ‹JSé‡å®šå‘
        if '<meta http-equiv="refresh"' in result.get('content', ''):
            return True
        # æ£€æµ‹å¸¸è§åçˆ¬æŠ€æœ¯
        anti_selectors = [
            '#challenge-form',  # Cloudflare
            '.geetest_captcha',  # Geetest
            '#px-captcha'  # PerimeterX
        ]
        soup = BeautifulSoup(result.get('content', ''), 'html.parser')
        return any(soup.select(selector) for selector in anti_selectors)

    def _success_result(self, url: str, content: Any, response=None) -> Dict:
        """æ„é€ æˆåŠŸå“åº”"""
        is_text = isinstance(content, str)
        hash_str = hashlib.md5(content.encode() if is_text else content).hexdigest()

        return {
            'url': url,
            'content': content,
            'content_type': response.headers.get('Content-Type', '') if response else 'text/html',
            'status': response.status if response else 200,
            'hash': hash_str,
            'attempts': 1
        }

    async def _handle_error(self, url: str, error: Exception, attempt: int):
        """ç»Ÿä¸€é”™è¯¯å¤„ç†"""
        self.logger.warning(f"Attempt {attempt + 1} failed for {url}: {str(error)}")
        backoff = self.config['retry_policy']['backoff_base'] ** attempt
        await asyncio.sleep(backoff + random.uniform(0, 1))

    def _failure_result(self, url: str) -> Dict:
        """æ„é€ å¤±è´¥å“åº”"""
        return {
            'url': url,
            'error': 'Max retries exceeded',
            'attempts': self.config['retry_policy']['max_attempts']
        }

    def extract_links(self, html: str, base_url: str) -> set:
        """ä»HTMLå†…å®¹ä¸­æå–èµ„æºé“¾æ¥"""
        soup = BeautifulSoup(html, 'html.parser')
        links = set()

        # æå–å›¾ç‰‡é“¾æ¥
        for img in soup.find_all('img'):
            src = img.get('src')
            if src:
                links.add(self.normalize_url(base_url, src))

        # æå–æ–‡ä»¶é“¾æ¥
        for a in soup.find_all('a'):
            href = a.get('href')
            if href and self.is_resource_link(href):
                links.add(self.normalize_url(base_url, href))

        return links

    def normalize_url(self, base_url: str, path: str) -> str:
        """è§„èŒƒåŒ–ç›¸å¯¹è·¯å¾„ä¸ºç»å¯¹URL"""
        parsed_base = urlparse(base_url)
        if path.startswith('//'):
            return f"{parsed_base.scheme}:{path}"
        elif path.startswith('/'):
            return f"{parsed_base.scheme}://{parsed_base.netloc}{path}"
        elif path.startswith(('http://', 'https://')):
            return path
        else:
            return f"{parsed_base.scheme}://{parsed_base.netloc}/{path}"

    def is_resource_link(self, url: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯èµ„æºé“¾æ¥"""
        for pattern in self.config['resource_patterns']:
            if re.match(pattern, url, re.IGNORECASE):
                return True
        return False

    async def process_result(self, result: Dict[str, Any]):
        """å¤„ç†æŠ“å–ç»“æœ"""
        if 'error' in result:
            self.logger.error(f"Failed to fetch {result['url']}: {result['error']}")
            return
        # å†…å®¹å»é‡æ£€æŸ¥
        cur = self.db_conn.execute(
            'SELECT url FROM pages WHERE content_hash = ?',
            (result['hash'],)
        )
        if cur.fetchone():
            self.logger.info(f"Skipping duplicate content: {result['url']}")
            return
        # ä¿å­˜æ–‡ä»¶
        try:
            file_path = self.save_to_file(
                result['url'],
                result['content'],
                result.get('content_type', 'text/html')
            )
        except Exception as e:
            self.logger.error(f"Failed to save {result['url']}: {str(e)}")
            return
        # å­˜å‚¨åˆ°æ•°æ®åº“
        self.db_conn.execute('''
			INSERT INTO pages (url, content_hash, file_path, status_code, attempts, content_type)
			VALUES (?, ?, ?, ?, ?, ?)
		''', (
            result['url'],
            result['hash'],
            file_path,
            result['status'],
            result['attempts'],
            result.get('content_type', '')
        ))
        self.db_conn.commit()
        # å¦‚æœæ˜¯HTMLå†…å®¹ï¼Œæå–èµ„æºé“¾æ¥
        if file_path.endswith('html'):
            try:
                html_content = result['content'].decode('utf-8') if isinstance(result['content'], bytes) else result[
                    'content']
                resource_links = self.extract_links(html_content, result['url'])
                await self.enqueue_resources(resource_links)
            except Exception as e:
                self.logger.error(f"Failed to extract links from {result['url']}: {str(e)}")

    async def enqueue_resources(self, links: set):
        """å°†èµ„æºé“¾æ¥åŠ å…¥é˜Ÿåˆ—"""
        for link in links:
            if not self.is_duplicate_url(link):
                await self.queue.put((link, True))

    def is_duplicate_url(self, url: str) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦å·²ç»å¤„ç†è¿‡"""
        cur = self.db_conn.execute(
            'SELECT url FROM pages WHERE url = ?',
            (url,)
        )
        return cur.fetchone() is not None

    def save_to_file(self, url: str, content: Any, content_type: str) -> str:
        """ä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿï¼ˆæ”¯æŒäºŒè¿›åˆ¶ï¼‰"""
        parsed_url = urlparse(url)
        path = parsed_url.path.strip('/') or 'index'

        # ç”Ÿæˆå®‰å…¨æ–‡ä»¶å
        safe_filename = re.sub(r'[^\w\-_.]', '_', path.split('/')[-1][:128])
        safe_filename = os.path.splitext(safe_filename)[0]

        file_ext = content_type.split('/')[-1].split(';')[0].split('+')[0]

        safe_filename += f'.{file_ext}'

        # æ„å»ºå­˜å‚¨è·¯å¾„
        category = self.get_file_category(file_ext)
        full_path = os.path.join(
            self.config['storage']['base_dir'],
            self.timestamp,
            category,
            parsed_url.netloc,
            safe_filename
        )
        os.makedirs(os.path.dirname(full_path), exist_ok=True)

        # å†™å…¥æ–‡ä»¶
        mode = 'wb' if isinstance(content, bytes) else 'w'
        encoding = None if isinstance(content, bytes) else 'utf-8'

        with open(full_path, mode, encoding=encoding) as f:
            f.write(content)

        return full_path

    def get_file_category(self, ext: str) -> str:
        """è·å–æ–‡ä»¶åˆ†ç±»ç›®å½•"""
        ext = ext.lower()
        for category, exts in self.config['file_types'].items():
            if ext in exts:
                return category
        return 'others'

    async def run(self, urls: URLGenerator):
        """è¿è¡Œå¢å¼ºç‰ˆæŠ“å–ä»»åŠ¡"""
        async with aiohttp.ClientSession() as self.session:
            # åˆ›å»ºæ··åˆworker
            workers = [
                asyncio.create_task(self._mixed_worker())
                for _ in range(self.config['concurrency'] * 2)
            ]

            # å¡«å……é˜Ÿåˆ—
            async def populate_queue():
                for url in urls:
                    await self.queue.put((url, False))
                await self.queue.join()

            await populate_queue()

            # æ¸…ç†worker
            for w in workers:
                w.cancel()
            await asyncio.gather(*workers, return_exceptions=True)

    async def _mixed_worker(self):
        """æ··åˆæ¨¡å¼worker"""
        while True:
            url, isr = await self.queue.get()
            try:
                result = await self.fetch(url, isr)
                await self.process_result(result)
            finally:
                self.queue.task_done()


def cleanup_resources(args, config):
    """æ¸…ç†æ‰€æœ‰æŒä¹…åŒ–èµ„æº"""
    import shutil

    if args.all or args.database:
        """æ¸…ç†æ•°æ®åº“æ–‡ä»¶"""
        db_path = config['storage']['db_file']
        if os.path.exists(db_path):
            try:
                os.remove(db_path)
                print(f"ğŸ—‘ï¸ æ•°æ®åº“æ–‡ä»¶å·²åˆ é™¤: {db_path}")
            except PermissionError as e:
                print(f"âŒ æ— æ³•åˆ é™¤æ•°æ®åº“æ–‡ä»¶: {str(e)}")

    if args.all or args.log:
        """æ¸…ç†æ—¥å¿—æ–‡ä»¶"""
        log_file = config['logging']['log_filename']
        for single_file in [log_file] + [log_file + f'.{i}' for i in range(1, config['logging']['backup_count'] + 1)]:
            if os.path.exists(single_file):
                try:
                    os.remove(single_file)
                    print(f"ğŸ—‘ï¸ æ—¥å¿—æ–‡ä»¶å·²åˆ é™¤: {single_file}")
                except Exception as e:
                    print(f"âŒ æ¸…ç†æ—¥å¿—å¤±è´¥: {str(e)}")

    if args.all or args.download:
        """æ¸…ç†ä¸‹è½½ç›®å½•"""
        download_dir = config['storage']['base_dir']
        if os.path.exists(download_dir):
            try:
                shutil.rmtree(download_dir)
                print(f"ğŸ—‘ï¸ ä¸‹è½½ç›®å½•å·²æ¸…ç†: {download_dir}")
            except Exception as e:
                print(f"âŒ æ¸…ç†ä¸‹è½½ç›®å½•å¤±è´¥: {str(e)}")
        else:
            print(f"â„¹ï¸ ä¸‹è½½ç›®å½•ä¸å­˜åœ¨: {download_dir}")


async def run_crawler(config, url_input, time, force_render=False):
    """çˆ¬å–ä¸»ç¨‹åº"""
    os.makedirs(config['storage']['base_dir'], exist_ok=True)

    if url_input.get('force_render'):
        force_render = True

    # ç”ŸæˆURLåˆ—è¡¨
    try:
        urls = URLGenerator(url_input)
    except ValueError as e:
        print(f"Invalid parameters: {str(e)}")
        sys.exit(1)

    # è¿è¡ŒæŠ“å–ä»»åŠ¡
    async with AsyncScraper(config, time, force_render) as scraper:
        await scraper.run(urls)


class VarAction(argparse.Action):
    """è‡ªå®šä¹‰å‚æ•°å¤„ç†å™¨ï¼Œè§£æå½¢å¦‚ varname type=type,start=x,end=y,step=z çš„å­—ç¬¦ä¸²"""

    def __call__(self, parser, namespace, values, option_string=None):
        # åˆå§‹åŒ–å­˜å‚¨ç»“æ„
        if not hasattr(namespace, 'var_rules'):
            namespace.var_rules = {}

        try:
            # åˆ†å‰²å˜é‡åå’Œå‚æ•°éƒ¨åˆ†
            var_name, params_str = values.split(',', 1)

            # è§£æå‚æ•°é”®å€¼å¯¹
            params = {}
            for pair in params_str.split(','):
                key, value = pair.split('=', 1)
                params[key.strip()] = value.strip()

            # å‚æ•°å®Œæ•´æ€§æ ¡éªŒ
            required_keys = {'type', 'start', 'end', 'step'}
            if missing := required_keys - params.keys():
                raise ValueError(f"ç¼ºå°‘å¿…è¦å‚æ•°: {missing}")

            # ç±»å‹è½¬æ¢é€»è¾‘
            if params['type'] == 'date':
                params = self._process_date_params(params)
            elif params['type'] == 'number':
                params = self._process_number_params(params)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„ç±»å‹: {params['type']}")

            # å­˜å‚¨åˆ°å‘½åç©ºé—´
            namespace.var_rules[var_name] = params

        except Exception as e:
            parser.error(f"å‚æ•°è§£æé”™è¯¯: {str(e)}")

    def _process_date_params(self, params):
        """å¤„ç†æ—¥æœŸç±»å‹å‚æ•°"""
        if params['format']:
            format = params['format']
        else:
            format = "%Y-%m-%d"
        try:
            datetime.strptime(params['start'], format)
            datetime.strptime(params['end'], format)
            params['step'] = int(params['step'])
            params['format'] = format
        except ValueError:
            raise ValueError(f"æ—¥æœŸæ ¼å¼åº”ä¸º {format}ï¼Œæ­¥é•¿åº”ä¸ºæ•´æ•°")
        return params

    def _process_number_params(self, params):
        """å¤„ç†æ•°å­—ç±»å‹å‚æ•°"""
        try:
            params['start'] = int(params['start'])
            params['end'] = int(params['end'])
            params['step'] = int(params['step'])
        except ValueError:
            raise ValueError("æ•°å€¼å‚æ•°åº”ä¸ºæ•°å­—")
        return params


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(description='Batch Web Scraper')
    parser.add_argument('-c', '--config', help='Configuration file path')
    subparsers = parser.add_subparsers(dest='command')

    run_parser = subparsers.add_parser('run', description='Run crawl process')
    run_parser.add_argument_group()
    run_parser.add_argument('-i', '--input', help='Url pattern and vars input file path')
    run_parser.add_argument('-p', '--pattern', help='URL pattern with placeholders {var}')
    run_parser.add_argument('-a', '--var-rule', action=VarAction, metavar="VAR_RULE",
                            help='Var rule, e.g. date,type=date,start=20240101,end=20240105,step=1,format=%%Y-%%m-%%d')
    run_parser.add_argument('-f', '--force-render', action='store_true', default=False,
                            help='Enable forced using browser render engine')

    clean_parser = subparsers.add_parser('clean', description='Clean generated and downloaded files')
    clean_group = clean_parser.add_mutually_exclusive_group(required=True)
    clean_group.add_argument('-a', '--all',
                             action='store_true',
                             help='Clean all log, downloaded and db file')
    clean_group.add_argument('-o', '--log',
                             action='store_true',
                             help='Clean log files')
    clean_group.add_argument('-d', '--download',
                             action='store_true',
                             help='Clean download folder')
    clean_group.add_argument('-D', '--database',
                             action='store_true',
                             help='Clean database file')

    args = parser.parse_args()

    # åˆå§‹åŒ–é…ç½®
    if args.config and os.path.exists(args.config):
        with open(args.config, 'r', encoding='utf-8') as f:
            try:
                config = json.load(f)
            except json.JSONDecodeError as e:
                print(f"é…ç½®æ–‡ä»¶è§£æé”™è¯¯: {str(e)}")
                sys.exit(1)
    else:
        config = DEFAULT_CONFIG

    if args.command == 'clean':
        print("ğŸš€ å¼€å§‹æ¸…ç†ç³»ç»Ÿèµ„æº...")
        cleanup_resources(args, config)
        print("âœ… èµ„æºæ¸…ç†å®Œæˆ\n")
    elif args.command == 'run':
        if args.pattern and args.input:
            print("Command input and file input must be set up to one")
            return
        if args.pattern is None and args.input is None:
            print("Command input and file input must be set at least one")
            return

        nowtime = datetime.now().strftime("%Y%m%d%H%M%S")
        print("Start doing crawling job...")
        # åˆ›å»ºæ–°äº‹ä»¶å¾ªç¯ï¼ˆæ˜¾å¼æŒ‡å®šï¼‰
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)  # ç»‘å®šåˆ°å½“å‰ä¸Šä¸‹æ–‡

        try:
            if args.pattern:
                _input = {
                    "url_template": args.pattern,
                    "variables": args.var_rules if hasattr(args, 'var_rules') else None
                }
                loop.run_until_complete(run_crawler(config, _input, nowtime, args.force_render))
            else:
                with open(args.input, 'r', encoding='utf-8') as f:
                    inputs = json.load(f)
                for _input in inputs:
                    if 'url_template' not in _input or 'variables' not in _input:
                        print("Invalid input file format")
                        return
                    loop.run_until_complete(run_crawler(config, _input, nowtime))
                    print(f"Done crawling job for {_input['url_template']}")
        finally:
            loop.close()  # æ˜ç¡®æ¸…ç†èµ„æº
        print("Done crawling job")


if __name__ == '__main__':
    main()
