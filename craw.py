#!/usr/bin/env python3
"""
Batch Web Scraper with Auto-Retry and Smart Storage
"""

import os
import re
import sys
import aiohttp
import asyncio
import hashlib
import sqlite3
from datetime import datetime, timedelta
from urllib.parse import urlparse
from typing import Iterator, Dict, Any
from itertools import product
import logging
from logging.handlers import RotatingFileHandler
import argparse
import json
import magic

# Configuration
DEFAULT_CONFIG = {
    'concurrency': 5,
    'retry_policy': {
        'max_attempts': 3,
        'backoff_base': 2
    },
    'storage': {
        'base_dir': './downloads',
        'db_file': 'scraped_data.db',
        'max_file_size': 1024 * 1024 * 100  # 100MB
    },
    'logging': {
        'log_filename': 'scraper.log',
        'backup_count': 5,
        'max_bytes': 1024 * 5
    },
    'user_agent': 'Mozilla/5.0 (compatible; BatchScraper/1.0; +https://example.com/bot)'
}


class URLGenerator:
    """åŠ¨æ€URLç”Ÿæˆå™¨"""

    def __init__(self, config: Dict[str, Any]):
        """
        :param config: åŒ…å«æ¨¡æ¿å’Œç”Ÿæˆè§„åˆ™çš„å­—å…¸
        """
        self.template = config['url_template']
        self.rules = config['variables']
        self._validate_config()

    def _validate_config(self):
        """éªŒè¯é…ç½®å®Œæ•´æ€§"""
        # æå–æ¨¡æ¿å˜é‡
        variables = set(re.findall(r'\{(.*?)}', self.template))

        # éªŒè¯è§„åˆ™åŒ¹é…
        missing = variables - set(self.rules.keys())
        if missing:
            raise ValueError(f"ç¼ºå°‘å˜é‡è§„åˆ™: {missing}")

        # éªŒè¯è§„åˆ™å‚æ•°
        required_params = {'type', 'start', 'end', 'step'}
        for var, rule in self.rules.items():
            if missing_params := required_params - set(rule.keys()):
                raise ValueError(f"å˜é‡ {var} ç¼ºå°‘å‚æ•°: {missing_params}")

    def _gen_numbers(self, rule: Dict) -> Iterator[int]:
        """ç”Ÿæˆæ•°å€¼åºåˆ—"""
        current = rule['start']
        end = rule['end']
        step = rule['step']

        while current <= end:
            yield current
            current += step

    def _gen_dates(self, rule: Dict) -> Iterator[str]:
        """ç”Ÿæˆæ—¥æœŸåºåˆ—"""
        format = str(rule['format'])
        start = datetime.strptime(str(rule['start']), format)
        end = datetime.strptime(str(rule['end']), format)
        step = timedelta(days=rule['step'])

        current = start
        while current <= end:
            yield current.strftime(format)
            current += step

    def _generate_values(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ‰€æœ‰å˜é‡çš„å€¼åºåˆ—"""
        value_generators = {}

        for var, rule in self.rules.items():
            if rule['type'] == 'number':
                value_generators[var] = self._gen_numbers(rule)
            elif rule['type'] == 'date':
                value_generators[var] = self._gen_dates(rule)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„å˜é‡ç±»å‹: {rule['type']}")

        return value_generators

    def __iter__(self) -> Iterator[str]:
        """ç”Ÿæˆå®Œæ•´çš„URLåºåˆ—"""
        values = self._generate_values()

        # ç”Ÿæˆç¬›å¡å°”ç§¯
        all_combinations = product(*[
            list(gen) for gen in values.values()
        ])

        for combination in all_combinations:
            params = dict(zip(values.keys(), combination))
            yield self.template.format(**params)


class AsyncScraper:
    """å¼‚æ­¥æŠ“å–å¼•æ“"""

    def __init__(self, config: Dict[str, Any], timestamp):
        self.config = config
        self.session = None
        self.semaphore = asyncio.Semaphore(config['concurrency'])
        self.logger = self.setup_logger()
        self.db_conn = sqlite3.connect(config['storage']['db_file'])
        self.timestamp = timestamp
        self.init_db()

    def init_db(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        with self.db_conn:
            self.db_conn.execute('''
                CREATE TABLE IF NOT EXISTS pages (
                    id INTEGER PRIMARY KEY,
                    url TEXT,
                    content_hash TEXT,
                    file_path TEXT,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                    status_code INTEGER,
                    attempts INTEGER
                )
            ''')
            self.db_conn.execute('''
                CREATE INDEX IF NOT EXISTS idx_url ON pages (url)
            ''')

    def setup_logger(self):
        """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
        logger = logging.getLogger('BatchScraper')
        logger.setLevel(logging.INFO)

        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )

        file_handler = RotatingFileHandler(
            self.config['logging']['log_filename'],
            maxBytes=self.config['logging']['max_bytes'],
            backupCount=self.config['logging']['backup_count']
        )
        file_handler.setFormatter(formatter)

        logger.addHandler(file_handler)
        return logger

    async def fetch(self, url: str) -> Dict[str, Any]:
        """æ‰§è¡Œå•ä¸ªè¯·æ±‚"""
        async with self.semaphore:
            for attempt in range(self.config['retry_policy']['max_attempts']):
                try:
                    async with self.session.get(
                            url,
                            headers={'User-Agent': self.config['user_agent']},
                            timeout=aiohttp.ClientTimeout(total=30)
                    ) as response:
                        content = await response.text()
                        content_hash = hashlib.md5(content.encode()).hexdigest()

                        return {
                            'url': url,
                            'content': content,
                            'status': response.status,
                            'hash': content_hash,
                            'attempts': attempt + 1
                        }
                except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                    self.logger.warning(f"Attempt {attempt + 1} failed for {url}: {str(e)}")
                    await asyncio.sleep(self.config['retry_policy']['backoff_base'] ** attempt)
            return {'url': url, 'error': str(e), 'attempts': attempt + 1}

    async def worker(self, queue: asyncio.Queue):
        """å·¥ä½œåç¨‹"""
        while True:
            url = await queue.get()
            try:
                result = await self.fetch(url)
                await self.process_result(result)
            finally:
                queue.task_done()

    async def process_result(self, result: Dict[str, Any]):
        """å¤„ç†æŠ“å–ç»“æœ"""
        if 'error' in result:
            self.logger.error(f"Failed to fetch {result['url']}: {result['error']}")
            return

        # å†…å®¹å»é‡æ£€æŸ¥
        cur = self.db_conn.execute(
            'SELECT url FROM pages WHERE content_hash = ?',
            (result['hash'],)
        )
        if cur.fetchone():
            self.logger.info(f"Skipping duplicate content: {result['url']}")
            return

        # å­˜å‚¨åˆ°æ–‡ä»¶ç³»ç»Ÿ
        file_path = self.save_to_file(result['url'], str(result['content']))

        # å­˜å‚¨åˆ°æ•°æ®åº“
        self.db_conn.execute('''
            INSERT INTO pages (url, content_hash, file_path, status_code, attempts)
            VALUES (?, ?, ?, ?, ?)
        ''', (
            result['url'],
            result['hash'],
            file_path,
            result['status'],
            result['attempts']
        ))
        self.db_conn.commit()

    def save_to_file(self, url: str, content: str) -> str:
        """ä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿ"""
        parsed_url = urlparse(url)
        path = parsed_url.path.strip('/') or 'index'
        safe_filename = re.sub(r'[^\w\-_.]', '_', path)

        filetype = magic.from_buffer(content, mime=True).split('/')[-1]
        if not safe_filename.endswith(f'.{filetype}'):
            safe_filename += f'.{filetype}'
        full_path = os.path.join(
            self.config['storage']['base_dir'],
            self.timestamp,
            parsed_url.netloc,
            safe_filename
        )

        os.makedirs(os.path.dirname(full_path), exist_ok=True)
        with open(full_path, 'w', encoding='utf-8') as f:
            f.write(content)
        return full_path

    async def run(self, urls: URLGenerator):
        """è¿è¡ŒæŠ“å–ä»»åŠ¡"""
        queue = asyncio.Queue(maxsize=1000)

        # åˆ›å»ºworkerä»»åŠ¡
        workers = [
            asyncio.create_task(self.worker(queue))
            for _ in range(self.config['concurrency'])
        ]

        # å¡«å……ä»»åŠ¡é˜Ÿåˆ—
        async def populate_queue():
            for url in urls:
                await queue.put(url)
            await queue.join()

        async with aiohttp.ClientSession() as self.session:
            await populate_queue()

        # æ¸…ç†worker
        for worker in workers:
            worker.cancel()
        await asyncio.gather(*workers, return_exceptions=True)


def cleanup_resources(args, config):
    """æ¸…ç†æ‰€æœ‰æŒä¹…åŒ–èµ„æº"""
    import shutil

    if args.all or args.database:
        """æ¸…ç†æ•°æ®åº“æ–‡ä»¶"""
        db_path = config['storage']['db_file']
        if os.path.exists(db_path):
            try:
                os.remove(db_path)
                print(f"ğŸ—‘ï¸ æ•°æ®åº“æ–‡ä»¶å·²åˆ é™¤: {db_path}")
            except PermissionError as e:
                print(f"âŒ æ— æ³•åˆ é™¤æ•°æ®åº“æ–‡ä»¶: {str(e)}")

    if args.all or args.log:
        """æ¸…ç†æ—¥å¿—æ–‡ä»¶"""
        log_file = config['logging']['log_filename']
        for single_file in [log_file] + [log_file + f'.{i}' for i in range(1, config['logging']['backup_count'] + 1)]:
            if os.path.exists(single_file):
                try:
                    os.remove(single_file)
                    print(f"ğŸ—‘ï¸ æ—¥å¿—æ–‡ä»¶å·²åˆ é™¤: {single_file}")
                except Exception as e:
                    print(f"âŒ æ¸…ç†æ—¥å¿—å¤±è´¥: {str(e)}")

    if args.all or args.download:
        """æ¸…ç†ä¸‹è½½ç›®å½•"""
        download_dir = config['storage']['base_dir']
        if os.path.exists(download_dir):
            try:
                shutil.rmtree(download_dir)
                print(f"ğŸ—‘ï¸ ä¸‹è½½ç›®å½•å·²æ¸…ç†: {download_dir}")
            except Exception as e:
                print(f"âŒ æ¸…ç†ä¸‹è½½ç›®å½•å¤±è´¥: {str(e)}")
        else:
            print(f"â„¹ï¸ ä¸‹è½½ç›®å½•ä¸å­˜åœ¨: {download_dir}")


def run_crawler(config, url_input, time):
    """çˆ¬å–ä¸»ç¨‹åº"""
    os.makedirs(config['storage']['base_dir'], exist_ok=True)

    # ç”ŸæˆURLåˆ—è¡¨
    try:
        urls = URLGenerator(url_input)
    except ValueError as e:
        print(f"Invalid parameters: {str(e)}")
        sys.exit(1)

    # è¿è¡ŒæŠ“å–ä»»åŠ¡
    scraper = AsyncScraper(config, time)

    # åˆ›å»ºæ–°äº‹ä»¶å¾ªç¯ï¼ˆæ˜¾å¼æŒ‡å®šï¼‰
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)  # ç»‘å®šåˆ°å½“å‰ä¸Šä¸‹æ–‡

    try:
        loop.run_until_complete(scraper.run(urls))
    finally:
        loop.close()  # æ˜ç¡®æ¸…ç†èµ„æº


class VarAction(argparse.Action):
    """è‡ªå®šä¹‰å‚æ•°å¤„ç†å™¨ï¼Œè§£æå½¢å¦‚ varname type=type,start=x,end=y,step=z çš„å­—ç¬¦ä¸²"""

    def __call__(self, parser, namespace, values, option_string=None):
        # åˆå§‹åŒ–å­˜å‚¨ç»“æ„
        if not hasattr(namespace, 'var_rules'):
            namespace.var_rules = {}

        try:
            # åˆ†å‰²å˜é‡åå’Œå‚æ•°éƒ¨åˆ†
            var_name, params_str = values.split(',', 1)

            # è§£æå‚æ•°é”®å€¼å¯¹
            params = {}
            for pair in params_str.split(','):
                key, value = pair.split('=', 1)
                params[key.strip()] = value.strip()

            # å‚æ•°å®Œæ•´æ€§æ ¡éªŒ
            required_keys = {'type', 'start', 'end', 'step'}
            if missing := required_keys - params.keys():
                raise ValueError(f"ç¼ºå°‘å¿…è¦å‚æ•°: {missing}")

            # ç±»å‹è½¬æ¢é€»è¾‘
            if params['type'] == 'date':
                params = self._process_date_params(params)
            elif params['type'] == 'number':
                params = self._process_number_params(params)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„ç±»å‹: {params['type']}")

            # å­˜å‚¨åˆ°å‘½åç©ºé—´
            namespace.var_rules[var_name] = params

        except Exception as e:
            parser.error(f"å‚æ•°è§£æé”™è¯¯: {str(e)}")

    def _process_date_params(self, params):
        """å¤„ç†æ—¥æœŸç±»å‹å‚æ•°"""
        if params['format']:
            format = params['format']
        else:
            format = "%Y-%m-%d"
        try:
            datetime.strptime(params['start'], format)
            datetime.strptime(params['end'], format)
            params['step'] = int(params['step'])
            params['format'] = format
        except ValueError:
            raise ValueError(f"æ—¥æœŸæ ¼å¼åº”ä¸º {format}ï¼Œæ­¥é•¿åº”ä¸ºæ•´æ•°")
        return params

    def _process_number_params(self, params):
        """å¤„ç†æ•°å­—ç±»å‹å‚æ•°"""
        try:
            params['start'] = int(params['start'])
            params['end'] = int(params['end'])
            params['step'] = int(params['step'])
        except ValueError:
            raise ValueError("æ•°å€¼å‚æ•°åº”ä¸ºæ•°å­—")
        return params


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(description='Batch Web Scraper')
    parser.add_argument('-c', '--config', help='Configuration file path')
    subparsers = parser.add_subparsers(dest='command')

    run_parser = subparsers.add_parser('run', description='Run crawl process')
    run_parser.add_argument_group()
    run_parser.add_argument('-i', '--input', help='Url pattern and vars input file path')
    run_parser.add_argument('-p', '--pattern', help='URL pattern with placeholders {var}')
    run_parser.add_argument('-a', '--var-rule', action=VarAction, metavar="VAR_RULE",
                            help='Var rule, e.g. date,type=date,start=20240101,end=20240105,step=1,format=%%Y-%%m-%%d')

    clean_parser = subparsers.add_parser('clean', description='Clean generated and downloaded files')
    clean_group = clean_parser.add_mutually_exclusive_group(required=True)
    clean_group.add_argument('-a', '--all',
                              action='store_true',
                              help='Clean all log, downloaded and db file')
    clean_group.add_argument('-o', '--log',
                             action='store_true',
                              help='Clean log files')
    clean_group.add_argument('-d', '--download',
                             action='store_true',
                              help='Clean download folder')
    clean_group.add_argument('-D', '--database',
                             action='store_true',
                              help='Clean database file')

    args = parser.parse_args()

    # åˆå§‹åŒ–é…ç½®
    if(args.config and os.path.exists(args.config)):
        with open(args.config, 'r', encoding='utf-8') as f:
            try:
                config = json.load(f)
            except json.JSONDecodeError as e:
                print(f"é…ç½®æ–‡ä»¶è§£æé”™è¯¯: {str(e)}")
                sys.exit(1)
    else:
        config = DEFAULT_CONFIG

    if args.command == 'clean':
        print("ğŸš€ å¼€å§‹æ¸…ç†ç³»ç»Ÿèµ„æº...")
        cleanup_resources(args, config)
        print("âœ… èµ„æºæ¸…ç†å®Œæˆ\n")
    elif args.command == 'run':
        if args.pattern and args.input:
            print("Command input and file input must be set up to one")
            return
        if args.pattern is None and args.input is None:
            print("Command input and file input must be set at least one")
            return

        nowtime = datetime.now().strftime("%Y%m%d%H%M%S")
        print("Start doing crawling job...")
        if args.pattern:
            input = {
                "url_template": args.pattern,
                "variables": args.var_rules
            }
            run_crawler(config, input, nowtime)
        else:
            with open(args.input, 'r', encoding='utf-8') as f:
                inputs = json.load(f)
            for input in inputs:
                if 'url_template' not in input or 'variables' not in input:
                    print("Invalid input file format")
                    return
                run_crawler(config, input, nowtime)
                print(f"Done crawling job for {input['url_template']}")
        print("Done crawling job")


if __name__ == '__main__':
    main()

